* weakly consistent model: Perhaps what we want is a system where we can write code that doesn't use expensive coordination, and yet returns a "usable" value. Instead of having a single truth, we will allow different replicas to diverge from each other - both to keep things efficient but also to tolerate partitions - and then try to find a way to deal with the divergence in some manner.
* **Eventual consistency**: nodes can for some time diverge from each other, but that eventually they will agree on the value.
	* Eventual consistency with probabilistic guarantees. This type of system can detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution. In other words, conflicting updates will sometimes result in overwriting a newer value with an older one. Amazon's Dynamo is an example of a system that offers eventual consistency with probabilistic guarantees.
	* Eventual consistency with strong guarantees. This type of system guarantees that the results converge to a common value equivalent to some correct sequential execution. In other words, such systems do not produce any anomalous results; CRDT's (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering. They are provably convergent, but the data types that can be implemented as CRDT's are limited. The CALM (consistency as logical monotonicity) conjecture is an alternative expression of the same principle: it equates logical monotonicity with convergence. If we can conclude that something is logically monotonic, then it is also safe to run without coordination.
* Amazon's **Dynamo**: basis of Facebook's Cassandra
	* Dynamo is a key-value store. A key value store is like a large hash table: a client can set values via set(key, value) and retrieve them by key using get(key). A Dynamo cluster consists of N peer nodes; each node has a set of keys. Dynamo prioritizes availability over consistency; it does not guarantee single-copy consistency. Instead, replicas may diverge from each other when values are written; when a key is read, there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client.
	* In Dynamo, keys are mapped to nodes using a hashing technique known as **consistent hashing**. The main idea is that a key can be mapped to a set of nodes responsible for it by a simple calculation on the client. This means that a client can locate keys without having to query the system for the location of each key; this saves system resources as hashing is generally faster than performing a remote procedure call.
	* Once we know where a key should be stored, we need to do some work to persist the value. This is a synchronous task; the reason why we will immediately write the value onto multiple nodes is to provide a higher level of durability (e.g. protection from the immediate failure of a node). Just like Paxos or Raft, Dynamo uses quorums for replication. However, Dynamo's quorums are sloppy (partial) quorums rather than strict (majority) quorums. The user can choose the number of nodes to write to and read from: the user can choose some number W-of-N nodes required for a write to succeed; and the user can specify the number of nodes (R-of-N) to be contacted during a read. W and R specify the number of nodes that need to be involved to a write or a read. Writing to more nodes makes writes slightly slower but increases the probability that the value is not lost; reading from more nodes increases the probability that the value read is up to date. The usual recommendation is that R + W > N, because this means that the read and write quorums overlap in one node - making it *less likely* that a stale value is returned. R + W > N does not mean strong consistency.
	* Systems that allow replicas to diverge must have a way to eventually reconcile two different values. One way to do this is to detect conflicts at read time, and then apply some conflict resolution method. But how is this done? In general, this is done by tracking the causal history of a piece of data by supplementing it with some metadata. Clients must keep the metadata information when they read data from the system, and must return back the metadata value when writing to the database. vector clocks can be used to represent the history of a value. Indeed, this is what the original Dynamo design uses for detecting conflicts.
	* When reading a value, the client contacts R of N nodes and asks them for the latest value for a key. It takes all the responses, discards the values that are strictly older (using the vector clock value to detect this). If there is only one unique vector clock + value pair, it returns that. If there are multiple vector clock + value pairs that have been edited concurrently (e.g. are not comparable), then all of those values are returned. This means that the client / application developer must occasionally handle these cases by picking a value based on some use-case specific criterion.
	* besides read repair, there is another synchronization of replica in Dynamo: Gossip is a probabilistic technique for synchronizing replicas. The pattern of communication (e.g. which node contacts which node) is not determined in advance. Instead, nodes have some probability p of attempting to synchronize with each other. Every t seconds, each node picks a node to communicate with. This provides an additional mechanism beyond the synchronous task (e.g. the partial quorum writes) which brings the replicas up to date. Compared with read repair, this is active synchronization.
	* to sum up, Dynamo system design:
		* consistent hashing to determine key placement
		* partial quorums for reading and writing
		* conflict detection and read repair via vector clocks and
		* gossip for replica synchronization
	* probabilistically bounded staleness (PBS) is an experimental measurement of inconsistency;
* It turns out that there is no known technique for making **string concatenation** resolve to the same value without imposing an order on the operations (e.g. without expensive coordination). However, there are operations which can be applied safely in any order, for example, max operation over integers, or sum operations.
* **CRDT**(informally): knowledge about data types and operations on it which is order independent;
* **CALM theorum**: logically monotonic programs are guaranteed to be eventually consistent. Then, if we know that some computation is logically monotonic, then we know that it is also safe to execute without coordination. **Formally, logical monotony means any inferences made within a framework such as first-order logic, once deductively valid, cannot be invalidated by new information. In a non-monotonic logic system, some conclusions can be invalidated by learning new knowledge**. If we can express our computation in a manner in which it is possible to test for monotonicity, then we can perform a whole-program static analysis that detects which parts of the program are eventually consistent and safe to run without coordination (the monotonic parts) - and which parts are not (the non-monotonic ones).
* In everyday reasoning, we make what is known as the **open-world assumption**: we assume that we do not know everything, and hence cannot make conclusions from a lack of knowledge. That is, any sentence may be true, false or unknown.