Computations primarily benefit from high-end hardware to the extent to which they can replace slow network accesses with internal memory accesses. The performance advantage of high-end hardware is limited in tasks that require large amounts of communication between nodes.

Scalability:
* adding more nodes should make the system linearly faster;
* growing the dataset should not increase latency
* it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner.
* adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).

Measure of scalability:
* performance
	* Short response time/low latency for a given piece of work
	* High throughput
	* Low utilization of computing resource(s)

* availability

A system that makes weaker guarantees has more freedom of action, and hence
potentially greater performance - but it is also potentially hard to reason
about. People are better at reasoning about systems that work like a single
system, rather than a collection of nodes.

network partitions (e.g. total network failure between some nodes)

There are two basic techniques that can be applied to a data set. It can be
split over multiple nodes (partitioning) to allow for more parallel processing.
It can also be copied or cached on different nodes to reduce the distance
between the client and the server and for greater fault tolerance (replication).
* partition
	* Partitioning improves performance by limiting the amount of data to be
	examined and by locating related data in the same partition
	* Partitioning improves availability by allowing partitions to fail
	independently, increasing the number of nodes that need to fail before
	availability is sacrificed
* replicate
	* Replication improves performance by making additional computing power and
	bandwidth applicable to a new copy of the data
	* Replication improves availability by creating additional copies of the data,
	increasing the number of nodes that need to fail before availability is
	sacrificed

Replication is also the source of many of the problems

Only one consistency model for replication - strong consistency - allows you to
program as-if the underlying data was not replicated. Other consistency models
expose some internals of the replication to the programmer. However, weaker
consistency models can provide lower latency and higher availability - and are
not necessarily harder to understand, just different.
