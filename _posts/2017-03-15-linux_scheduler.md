## Linux Scheduler
===================
* For o(1) scheduler, when does the preempt occur?
	* A running process on CPU can wake up another process, and it would mark itself as 'should-be-preempted' in its `thread_info` struct, next time when CPU returns from interrupt handling, it would check whether the running task is marked as to be preempted, if yes, it would trigger the scheduler code;
	* For SMP, tasks on other CPUs can mark the `thread_info` of another CPU;
* For o(1) scheduler, why it does not support interactive tasks well basically?
	* o(n) scheduler does well because it re-calculate the time slice of ALL tasks, including those on the wait queue, so the time slice of interactive tasks would increase;
	* o(1) scheduler re-calculate the time slice of task when the task uses up its time slice, so it does not consider the wait queue basically.
	* Q: How does time slice computed?
	* o(1) scheduler uses historical sleep time to try to pick out interactive tasks, and give it a chance to stay in the active array after time slice expiration.
* `thread_info` vs `task_struct`
	* Prior to the Linux 2.6 kernel, struct `task_struct` was present at the end of the kernel stack of each process. There was no `thread_info` struct concept. But in Linux 2.6 kernel, instead of `task_struct` being placed at the end of the kernel stack for the process, the `thread_info` struct is placed at the end. This `thread_info` struct contains a pointer to the `task_struct` structure.
	* the reason is: `task_struct` is huge. it's around 1.7KB on a 32 bit machine. on the other hand, you can easily see that `thread_info` is much slimmer. kernel stack is either 4 or 8KB, and either way a 1.7KB is pretty much, so storing a slimmer struct, that points to `task_struct`, immediately saves a lot of stack space and is a scalable solution.
* For realtime processes, o(1) scheduler, for normal processes, CFS is used after kernel 2.6.22
* CFS does not distinguish interactive task, but it works well, because it has no concept of time slice, when a task is in wait queue, its exec time is small, so its virtual time is small, so it would be in the left-down corner of the red-black tree
* If a kernel has preempt feature(o(1) or CFS), then during the compilation of kernel, we can specify whether we want to enable `disable-preempt` feature by `configure`. If kernel has enabled `disable-preempt` feature, then one task can disable preempt when running on CPU by setting a variable in `thread_info` in kernel space. Everytime CPU returns from interrupt handling, it would check the preempt variable in `thread_info`, if it is disabled, **and the task is running in kernel context**, then even if scheduler decides it is time to schedule another task in, the switch does not really happen. Tasks usally disable preempt when they don't want to be scheduled out during the consequent code block.
* For a single CPU machine, spin lock in kernel can be implemented by disabling preempt for reading and writing of the lock flag. Yes, it can also be implemented by `XCHG`, but that relies on architecture, so kernel choose to use disabling preempt. For a SMP machine, spin lock in kernel is implemented by `LOCK` and `XCHG`, `LOCK` is to make SMP safe, `XCHG` is to make preempt safe. There is a memory arbitrator in SMP machines, but the arbitrator does not guarantee the order of 'read + write' from different CPUs as an atomic unit.
* In kernel, best practice is not sleeping when you hold a spin lock, because this may cause kernel PANIC: if a task acquires spin lock and goes to sleep, then another task is scheduled in, and this task disables preempt first(which is logically reasonable, if it does not want to be scheduled out for the following work), then tries to acquire the spin lock; of course, it cannot get the lock, so it would spin on the CPU in kernel context, then even if timer interrupt comes, it would not be scheduled out because it turns off preempt, so spin lock acquire would hit the timeout and raise kernel PANIC.